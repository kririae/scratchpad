{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2Khn2cJ7wjb"
      },
      "source": [
        "# Probability for Rendering\n",
        "\n",
        "Hi! Welcome to Computer Graphics at ShanghaiTech University!\n",
        "In this lab, we'll guide you through some simple yet important factors in building a _Physically-based Render_.\n",
        "It might also serve as an excuse for you to review probability theory.\n",
        "\n",
        "Please execute the following code section to make sure that all dependencies are installed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install numpy\n",
        "%pip install pandas\n",
        "%pip install scipy\n",
        "%pip install matplotlib\n",
        "%pip install seaborn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Rc1_LBCP7wje"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import statistics\n",
        "import math\n",
        "import random\n",
        "\n",
        "import scipy.integrate as integrate\n",
        "import scipy.special as special\n",
        "import scipy.optimize as optimize\n",
        "from scipy.stats import norm, truncnorm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn\n",
        "\n",
        "np.random.seed(42)\n",
        "seaborn.set_theme()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4D6GfrW7wjg"
      },
      "source": [
        "## Estimator Basics\n",
        "\n",
        "You might have heard of the words _biased_, _consistent_ or _unbiased_ along the way of learning probability.\n",
        "Generally, these words are used to describe _estimators_.\n",
        "**Estimators are random variables**. The reason they are called _estimators_ is that, they are used to estimate some _underlying values_.\n",
        "So for arbitrary _estimator_, we can take, for example, expectation and variance on them.\n",
        "Don't worry if you don't understand what do me mean by _underlying values_, by the end of this lab, you'll get a comprehensive understanding of estimator.\n",
        "\n",
        "Here is a simple and incomplete abstraction of normal distribution $\\mathcal{N}(\\mu_i, \\sigma_i^2)$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "wvhdUyO07wjj"
      },
      "outputs": [],
      "source": [
        "class Normal:\n",
        "    def __init__(self, n: int = 100) -> None:\n",
        "        self.n_ = n  # num samples\n",
        "        self.mu_ = math.sqrt(42)\n",
        "        self.sigma_ = 24\n",
        "        self.samples_ = np.random.normal(self.mu_, self.sigma_, self.n_)\n",
        "\n",
        "    def sample(self) -> float:\n",
        "        self.n_ -= 1\n",
        "        result = self.samples_[self.n_]\n",
        "        return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_g0VQLx-7wjk"
      },
      "source": [
        "Suppose that we're in a new universe with unknown light speed $c$, each sample (experiment) of the previous class will give us a _measure_ of light speed.\n",
        "You, our explorer, have limited resource to perform experiments, that you can only perform experiments for $100$ times.\n",
        "\n",
        "But you are asked by your commander to give the prediction of $c^2$, which is unexpectingly important in scientific computing.\n",
        "Your rough plan is to design an estimator $Y$ that can give you the prediction, i.e., $\\mathbb{E}\\left[Y\\right] = c^2$.\n",
        "Your commander told you that, _all estimator design begins with writing this form of formula_.\n",
        "\n",
        "After some rough thinking, you think there are three approaches to design the estimator.\n",
        "And you acknowledged that calling `sample` for $n$ times is equivalent to producing $n$ i.i.d. random variables $X_i$ where $\\mathbb{E}\\left[X_i\\right] = c$.\n",
        "For now, you don't have to understand what _biased_, etc. really means.\n",
        "\n",
        "$$\n",
        "\\left\\{\n",
        "\\begin{aligned}\n",
        "  Y_1 &= \\dfrac{1}{100}\\sum_{i = 1}^{100}{X_i^2} &\\text{(biased)} \\\\\n",
        "  Y_2 &= \\left(\\dfrac{1}{100}\\sum_{i = 1}^{100}{X_i}\\right)^2 &\\text{(consistent)} \\\\\n",
        "  Y_3 &= \\left(\\dfrac{1}{50}\\sum_{i = 1}^{50}{X_i}\\right) \\times \\left(\\dfrac{1}{50}\\sum_{i = 51}^{100}{X_i}\\right) &\\text{(unbiased)} \\\\\n",
        "\\end{aligned}\n",
        "\\right.\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTsBcskq7wjl"
      },
      "source": [
        "As an appetizer, your task is to fill the following Python functions with the formulas, which will give us the estimation of $c^2$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "juoBevy37wjm"
      },
      "outputs": [],
      "source": [
        "def biased(normal: Normal) -> float:\n",
        "    result = []\n",
        "    # TODO: your implementation of the *biased* estimator above\n",
        "    for _ in range(100):\n",
        "        result.append(math.pow(normal.sample(), 2))\n",
        "    return statistics.mean(result)\n",
        "\n",
        "\n",
        "def consistent(normal: Normal) -> float:\n",
        "    result = []\n",
        "    # TODO: your implementation of the *consistent* estimator above\n",
        "    for _ in range(100):\n",
        "        result.append(normal.sample())\n",
        "    return math.pow(statistics.mean(result), 2)\n",
        "\n",
        "\n",
        "def unbiased(normal: Normal) -> float:\n",
        "    # TODO: your implementation of the *unbiased* estimator above\n",
        "    # First estimator\n",
        "    result = []\n",
        "    for _ in range(50):\n",
        "        result.append(normal.sample())\n",
        "    I1 = statistics.mean(result)\n",
        "\n",
        "    # Second estimator\n",
        "    result = []\n",
        "    for _ in range(50):\n",
        "        result.append(normal.sample())\n",
        "    I2 = statistics.mean(result)\n",
        "\n",
        "    return I1 * I2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwqFfOgB7wjo"
      },
      "source": [
        "Great! As a numerical verification, execute the following code,\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qIPjFjm-7wjq",
        "outputId": "15a7d453-7c6c-40c1-ae7f-f692e627bc90"
      },
      "outputs": [],
      "source": [
        "# A sample-based mean/variance estimator\n",
        "def test(func, n: int = 1000) -> float:\n",
        "    result = []\n",
        "    for _ in range(n):\n",
        "        result.append(func())\n",
        "    return (statistics.mean(result), statistics.variance(result))\n",
        "\n",
        "\n",
        "pd.options.display.float_format = \"{:.2f}\".format\n",
        "df = pd.DataFrame(\n",
        "    index=[\"biased\", \"consistent\", \"unbiased\"], columns=[\"mean\", \"variance\"]\n",
        ")\n",
        "df.loc[\"biased\"] = test(lambda: biased(normal=Normal()))\n",
        "df.loc[\"consistent\"] = test(lambda: consistent(normal=Normal()))\n",
        "df.loc[\"unbiased\"] = test(lambda: unbiased(normal=Normal()))\n",
        "print(df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzyaGWRk7wjt"
      },
      "source": [
        "The performance discrepancy is obvious since the ground truth of $c^2$ is $42$, only the _unbiased_ estimator approaches the result.\n",
        "\n",
        "But why is that?\n",
        "\n",
        "Sure, let's take their expectation respectively to observe the result. Recall that,\n",
        "\n",
        "$$\n",
        "  \\dfrac{1}{n}\\sum_{i = 1}^{n} \\mathcal{N}(\\mu_i, \\sigma_i^2) = \\mathcal{N}\\left(\\dfrac{1}{n}\\sum_{i = 1}^{n}{\\mu_i}, \\dfrac{1}{n^2}\\sum_{i = 1}^{n}{\\sigma_i^2}\\right)\n",
        "$$\n",
        "\n",
        "and\n",
        "\n",
        "$$\n",
        "  \\mathbb{V}\\left[X\\right] = \\mathbb{E}\\left[X^2\\right] - \\mathbb{E}^2\\left[X\\right]\n",
        "$$\n",
        "\n",
        "Then let's test our estimators,\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "  \\mathbb{E}\\left[Y_1\\right] &= \\dfrac{1}{100}\\sum_{i = 1}^{100}{\\mathbb{E}\\left[X_i^2\\right]} \\\\\n",
        "                             &= \\dfrac{1}{100}\\sum_{i = 1}^{100}{\\left(\\mathbb{V}\\left[X_i\\right] + \\mathbb{E}^2\\left[X_i\\right]\\right)} \\\\\n",
        "                             &=  c^2 + \\dfrac{1}{100}\\sum_{i = 1}^{100}{\\mathbb{V}\\left[X_i\\right]} \\approx 576 + 41 = 617\n",
        "\\end{aligned}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVO6mzIG7wju"
      },
      "source": [
        "At this point, I believe you have understood what _biased estimator_ means, which is, $\\mathbb{E}\\left[{Y_i}\\right] - c^2 > 0$, and no matter how many more samples we obtain, the bias will be pertained, which is the variance of all samples.\n",
        "\n",
        "We could introduce the term _consistent estimator_ now. **Asymptotically**, when the number of samples we can obtain converges to $+\\infty$, $\\mathbb{E}\\left[Y_i\\right] = c^2$. Here is the example,\n",
        "Let $X = \\sum_{i} X_i = \\mathcal{N}\\left(\\overline{\\mu}, \\overline{\\sigma^2}/n\\right)$,\n",
        "\n",
        "$$\n",
        "  \\mathbb{E}\\left[Y_2\\right] = \\mathbb{E}\\left[X^2\\right] = \\mathbb{V}\\left[X\\right] + \\mathbb{E}^2\\left[X\\right] = c^2 + \\dfrac{\\sigma^2}{n}\n",
        "$$\n",
        "\n",
        "Then when $n \\rightarrow +\\infty$, the biased is eliminated, which corresponds to the definition of _consistent estimator_.\n",
        "However, whatever the parameter `n` of the `test` function is (not the `n` in `class Normal`, there is a crucial difference between these two $n$, why?), we will not obtain the correct result, since for a specific `n`, the estimator is biased at all.\n",
        "\n",
        "Consistent estimator is great! But we deserve somehow more. For the unbiased estimator, $\\forall n \\ge 1, \\mathbb{E}\\left[Y_3\\right] = c^2$. The proof is yet easy,\n",
        "\n",
        "$$\n",
        "  \\mathbb{E}\\left[Y_3\\right] = \\mathbb{E}\\left[I_1\\right] \\times \\mathbb{E}\\left[I_2\\right] = c^2.\n",
        "$$\n",
        "\n",
        "where $I_1$ and $I_2$ are the independent r.v.s generated from disjoint ranges of $X_i$.\n",
        "\n",
        "Before the end of this section, there is one thing that worth mentioning. _Biased_ do not necessarily imply that the estimator is inefficient.\n",
        "Although the previous example's bias is due to inadvertent wrong design.\n",
        "In real cases, sometimes bias are introduced intentionally to obtain low variance,\n",
        "sometimes bias are difficult to mitigate, or it takes a lot of effort to eliminate, so people just leave it as it is.\n",
        "Anyway, you'll meet a lot of biased but efficient estimators along the way, don't panic.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEEfcGMo7wjv"
      },
      "source": [
        "### Conclusion and Exercise\n",
        "\n",
        "Here's a simple conclusion. Strategy makes a difference. At this point, you should at least understand what are estimators, what are _biased_, _consistent_ and _unbiased_ estimators.\n",
        "As for designing estimators, which is an extremely difficult and interesting field (not just) in Computer Graphics, I'll present a more interesting example here.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSZHEZ_e7wjw"
      },
      "source": [
        "Your commander is very satisfied with your estimation, while traveling through a patch of interstellar dust, he asked you to give another _unbiased_ estimation to $\\exp(c)$, i.e., $e^c$.\n",
        "Unlike the last time, this time you have more resources to conduct more experiments.\n",
        "\n",
        "In introducing the techniques to solve this problem, we need to retrospect some basics.\n",
        "\n",
        "Suppose we have an estimator (r.v.) $X$. Recall that $X$ is a (measurable) map. Yes, random variables are not variables, they are maps.\n",
        "And there can have a _distribution_ defined on its _range_. Since $X$ is a map, and maps can be composed, i.e., $f \\circ g$.\n",
        "Any _valid_ functions applied on $X$ can form a new random variable.\n",
        "For example, $f(X)$ is yet another random variable.\n",
        "Further, $X \\cdot Y$ is a new r.v., $p(X)$ is yet r.v., where $p$ is its distribution function.\n",
        "\n",
        "Then, our estimator design for $\\exp(c)$ is $X$, where $\\mathbb{E}\\left[X\\right] = \\exp(c)$. Then, perform Taylor series expansion on $\\exp(c)$.\n",
        "\n",
        "$$\n",
        "  \\exp(c) = 1 + \\dfrac{c}{1!} + \\dfrac{c^2}{2!} + \\dfrac{c^2}{3!} + \\cdots \\; \\text{(countable series)}\n",
        "$$\n",
        "\n",
        "$X$ can be decomposed like this, $\\mathbb{E}\\left[X\\right] = 1 + \\sum_{i = 1}^{\\infty}{\\mathbb{E}\\left[X_i\\right]}$.\n",
        "So our task, actually, is to evaluate this infinite series. But we don't have infinitely many resources to use! Here is where the trick comes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAguQa3D7wjy"
      },
      "source": [
        "We let $\\mathbb{E}\\left[X_i\\right] = \\mathbb{E}\\left[I_i \\cdot X_i'\\right]$,\n",
        "where $I_i$ is an indicator variable controlling whether this term $X_i$ should be taken into the summation.\n",
        "Since $\\mathbb{E}\\left[I_i \\cdot X_i'\\right] = 1 \\cdot X_i' \\cdot P(I_i = 1)$, $X_i' = X_i P(I_i)^{-1}$.\n",
        "**For any** series of indicator variables $\\{I_i\\}_{i = 1}^{+\\infty}$, this infinite series will evaluate to a correct result.\n",
        "\n",
        "We somehow expect that the series' evaluation process can be terminated in some $i$, then let $K \\sim \\mathrm{Geo}(p)$, and $I_i = [K > i]$, written in [Iverson bracket](https://www.wikiwand.com/en/Iverson%20bracket), we obtain the unbiased estimator of $\\exp(c)$, which is\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "  \\mathbb{E}\\left[X\\right] &= \\exp(c) \\\\\n",
        "    &= 1 + \\sum_{i = 1}^{+\\infty}{I_i \\dfrac{X_i}{P(I_i = 1)}}\n",
        "    = 1 + \\sum_{i = 1}^{K-1}{\\dfrac{X_i}{P(K > i)}}\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "where $\\mathbb{E}\\left[X_i\\right] = \\dfrac{c^i}{i!}$, whose unbiased estimator design is known from our previous exercise, and $P(K > i) = (1 - p)^i, \\forall i \\ge 1$.\n",
        "\n",
        "Further, one way to obtain (sample) $K$ is to actually simulate the process, where _success_ means _termination_. Try to implement it here!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mx9S9vFy7wjz",
        "outputId": "77e62021-0a1a-4728-c32e-acf3db0a50e3"
      },
      "outputs": [],
      "source": [
        "class Normal:\n",
        "    def __init__(self, mu: float) -> None:\n",
        "        self.mu_ = mu\n",
        "        self.sigma_ = 1.0\n",
        "\n",
        "    def sample(self) -> float:\n",
        "        return np.random.normal(self.mu_, self.sigma_)\n",
        "\n",
        "\n",
        "def rr(func) -> float:\n",
        "    p = 0.1  # terminate with a prob 0.1\n",
        "    result = 1.0  # 1 + ... (in Taylor series)\n",
        "    i = 0\n",
        "    while True:\n",
        "        # TODO: fill in your implementation here\n",
        "        i += 1\n",
        "        if random.random() <= p:\n",
        "            break\n",
        "        result += func(i) / math.pow((1 - p), i)\n",
        "    return result\n",
        "\n",
        "\n",
        "def exp_series(func, i: int) -> float:\n",
        "    # TODO: fill in your implementation here\n",
        "    result = 1.0\n",
        "    for j in range(i):\n",
        "        result *= func() / (j + 1)\n",
        "    return result\n",
        "\n",
        "\n",
        "mu = 2.0\n",
        "print(\n",
        "    f\"the estimation of [exp({mu})={math.exp(mu):.4f}] is\",\n",
        "    test(lambda: rr(lambda i: exp_series(func=Normal(mu).sample, i=i)))[0],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQ7C7nBP7wj0"
      },
      "source": [
        "This approach is called [Russian Roulette](https://www.wikiwand.com/en/Russian_roulette), abbreviate to RR.\n",
        "RR is a such powerful tool to construct estimator on infinite series that you'll see its application later in your rendering homework.\n",
        "\n",
        "In conclusion, we've constructed an unbiased estimator to estimate the form like $\\exp(c)$ with RR.\n",
        "Note that we already have an unbiased estimator for $\\forall i \\ge 1, c^i$.\n",
        "$c$ is not necessarily a constant. We'll see an example later.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzKt-08W7wj1"
      },
      "source": [
        "## Monte Carlo Integration\n",
        "\n",
        "Monte Carlo Integration is a realization of _estimator design_ and _numerical integration_, which designs estimator on a broad range of integral.\n",
        "To understand how powerful the Monte Carlo Integration is, we start with the definition of _numerical integration_.\n",
        "\n",
        "Unlike our homework helper (_Mathematica_) in the mathematical analysis course, which is symbolic integration, _numerical integration_ intends to \"calculate the numerical value of a definite integral\".\n",
        "In 1-D case, we might want,\n",
        "\n",
        "$$\n",
        "  c = \\int_{a}^{b}{f(x) \\, \\mathrm{d} x},\n",
        "$$\n",
        "\n",
        "which can be abstracted to $I_{\\mathrm{numerical}}\\left[\\mathbb{R}, [a, b] \\subset \\mathbb{R}, f\\right] \\rightarrow \\mathbb{R}$, where $f$ might at least defined on $[a, b]$.\n",
        "Note that this function $I_{\\mathrm{numerical}}$ accepts several things, a _set_, the _integral domain_ and a _real-valued function_ on this domain.\n",
        "\n",
        "A common implementation of $I_{\\mathrm{numerical}}$ is to partition $[a, b]$ into disjoint _subintervals_, then perform some kinds of summations.\n",
        "We'll neglect the detail here because Monte Carlo Integration is somehow completely different.\n",
        "Those who are interested can refer to the explanation of numerical integration on [Wikipedia](https://www.wikiwand.com/en/Numerical_integration).\n",
        "\n",
        "To further introduce Monte Carlo Integration to you, you need to understand why we need it.\n",
        "It is what motivation that drives us away from these simple and intuitive numerical integration approaches?\n",
        "You will be amazed by its _power_, _elegance_, and _simplicity_.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "El6dAzaW7wj2"
      },
      "source": [
        "Again, Monte Carlo Integration is the realization of _estimator design_. You should ask two questions,\n",
        "\n",
        "- What is the underlying value?\n",
        "- How can we design the estimator?\n",
        "\n",
        "The underlying value, to simplify, is a definite integral where $c = \\int_{a}^{b}{f(x) \\, \\mathrm{d}x}$ (actually, to apply Monte Carlo Integration to Rendering, readers might need to understand integration on general measure space).\n",
        "Recall that we expect an estimator $Y$ such that,\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "  \\mathbb{E}\\left[Y\\right] &= c = \\int_{a}^{b}{f(x) \\, \\mathrm{d}x} \\\\\n",
        "    &= \\int_{a'}^{b'}{ y \\cdot p(y) \\, \\mathrm{d}y} \\\\\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "to align these two integrals, $a' = a, b' = b$, most importantly,\n",
        "\n",
        "$$\n",
        "  Y = \\dfrac{f(X)}{p(X)} = g(X).\n",
        "$$\n",
        "\n",
        "where $X$ can be **any** valid r.v. To verify,\n",
        "\n",
        "$$\n",
        "  \\mathbb{E}\\left[g(X)\\right] = \\int_{a'}^{b'}\\dfrac{f(x)}{\\cancel{p(x)}} \\cdot \\cancel{p(x)} \\, \\mathrm{d}x.\n",
        "$$\n",
        "\n",
        "Isn't it too simple? But it's this simple method that builds the field _Rendering_.\n",
        "\n",
        "Let's demonstrate this process with code.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzyo9xG57wj3"
      },
      "source": [
        "You and your fleet are moving through a known stretch of interstellar dust, again!\n",
        "You can your commander realized that you need to perform integration on the density of dust along the way you are traveling to estimate and reduce energy consumption, which is an integral like this,\n",
        "\n",
        "$$\n",
        "  c = \\int_{0}^{4.5}{\\sigma(t) \\, \\mathrm{d}t},\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48ZKpf5A7wj3"
      },
      "source": [
        "Random variables from now on are abstracted into a class, which is supposed to be easy to understand.\n",
        "We've provided three examples here, note that `Normal` distribution should not be used as the preceding random variable of Monte Carlo estimator,\n",
        "since it is not bounded, otherwise the estimator is biased.\n",
        "\n",
        "Estimators are generated by factory functions like `make_monte_carlo_estimator`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-H-v5_Fx7wj3",
        "outputId": "65914a9f-abae-45f6-d2cb-d87802ab9f06"
      },
      "outputs": [],
      "source": [
        "# https://docs.scipy.org/doc/scipy/tutorial/integrate.html\n",
        "# Use this simple but non-trivial function\n",
        "def func(x):\n",
        "    return special.jv(2.5, x)\n",
        "\n",
        "\n",
        "class RandomVariable:\n",
        "    \"\"\"\n",
        "    Definition of RandomVariable\n",
        "        Not a formal definition, but straight-forward\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def sample(self) -> float:\n",
        "        assert False\n",
        "\n",
        "    def pdf(self, x: float) -> float:\n",
        "        assert False\n",
        "\n",
        "    def expect(self, n: int = 1000) -> float:\n",
        "        # LLN\n",
        "        result = 0.0\n",
        "        for _ in range(n):\n",
        "            result += self.sample()\n",
        "        return result / n\n",
        "\n",
        "\n",
        "class Uniform(RandomVariable):\n",
        "    def __init__(self, a: float, b: float):\n",
        "        self.a_ = a\n",
        "        self.b_ = b\n",
        "        self.inv_length_ = 1 / (b - a)\n",
        "\n",
        "    def sample(self) -> float:\n",
        "        return np.random.uniform(low=self.a_, high=self.b_)\n",
        "\n",
        "    def pdf(self, x: float) -> float:\n",
        "        return self.inv_length_\n",
        "\n",
        "\n",
        "class Normal(RandomVariable):\n",
        "    def __init__(self, mu, sigma):\n",
        "        self.mu_ = mu\n",
        "        self.sigma_ = sigma\n",
        "        self.pdf_ = lambda x: norm.pdf(x, self.mu_, self.sigma_)\n",
        "\n",
        "    def sample(self) -> float:\n",
        "        return np.random.normal(self.mu_, self.sigma_)\n",
        "\n",
        "    def pdf(self, x: float) -> float:\n",
        "        return self.pdf_(x)\n",
        "\n",
        "\n",
        "class TruncNormal(RandomVariable):\n",
        "    def __init__(self, a: float, b: float):\n",
        "        self.a_ = a\n",
        "        self.b_ = b\n",
        "        self.mu_ = (a + b) / 2\n",
        "        self.sigma_ = b - a\n",
        "        self.cdf_ = lambda x: norm.cdf(x, self.mu_, self.sigma_)\n",
        "        self.pdf_ = lambda x: norm.pdf(x, self.mu_, self.sigma_)\n",
        "        self.factor_ = self.cdf_(b) - self.cdf_(a)\n",
        "\n",
        "    def sample(self) -> float:\n",
        "        # rejection sampling\n",
        "        sample = np.random.normal(self.mu_, self.sigma_)\n",
        "        while not (self.a_ <= sample <= self.b_):\n",
        "            sample = np.random.normal(self.mu_, self.sigma_)\n",
        "        return sample\n",
        "\n",
        "    def pdf(self, x: float) -> float:\n",
        "        return self.pdf_(x) / self.factor_\n",
        "\n",
        "\n",
        "class NumericalIntegral:\n",
        "    \"\"\"\n",
        "    An integral can be abstracted to\n",
        "        1. A function defined on Omega\n",
        "        2. A (measurable) set (indicated by low, high)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, func, low, high) -> None:\n",
        "        self.func_ = func\n",
        "        self.low_ = low\n",
        "        self.high_ = high\n",
        "        self.max_ = None\n",
        "        self.min_ = None\n",
        "\n",
        "    def get_value(self):\n",
        "        # Use general numerical integration method\n",
        "        assert isinstance(self.low_, float) and isinstance(self.high_, float)\n",
        "        return integrate.quad(self.func_, self.low_, self.high_)[0]\n",
        "\n",
        "    def get_max(self):\n",
        "        # Reserved for MCMC methods\n",
        "        assert isinstance(self.low_, float) and isinstance(self.high_, float)\n",
        "        # https://stackoverflow.com/questions/10146924/finding-the-maximum-of-a-function\n",
        "        if self.max_ is None:\n",
        "            self.max_ = -optimize.minimize_scalar(\n",
        "                lambda x: -self.func_(x),\n",
        "                bounds=[self.low_, self.high_],\n",
        "                method=\"bounded\",\n",
        "            ).fun\n",
        "        return self.max_\n",
        "\n",
        "    def get_min(self):\n",
        "        # Reserved for residual-tracking\n",
        "        assert isinstance(self.low_, float) and isinstance(self.high_, float)\n",
        "        # https://stackoverflow.com/questions/10146924/finding-the-maximum-of-a-function\n",
        "        if self.min_ is None:\n",
        "            self.min_ = optimize.minimize_scalar(\n",
        "                self.func_, bounds=[self.low_, self.high_], method=\"bounded\"\n",
        "            ).fun\n",
        "        return self.min_\n",
        "\n",
        "    def make_monte_carlo_estimator(self, rv_factory) -> RandomVariable:\n",
        "        class Estimator(RandomVariable):\n",
        "            def __init__(self_):\n",
        "                self_.base_rv_ = rv_factory(self.low_, self.high_)\n",
        "\n",
        "            def sample(self_) -> float:\n",
        "                # TODO: fill in your implementation here\n",
        "                spl = self_.base_rv_.sample()\n",
        "                return self.func_(spl) / self_.base_rv_.pdf(spl)\n",
        "\n",
        "        return Estimator()\n",
        "\n",
        "\n",
        "integral = NumericalIntegral(func=func, low=0.0, high=4.5)\n",
        "est_uniform = integral.make_monte_carlo_estimator(Uniform)\n",
        "est_tnormal = integral.make_monte_carlo_estimator(TruncNormal)\n",
        "print(f\"groundtruth: {integral.get_value():.4f}\")\n",
        "print(f\"uniform:     {est_uniform.expect():.4f}\")\n",
        "print(f\"tnormal:     {est_tnormal.expect():.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bo37FDvn7wj5"
      },
      "source": [
        "Hey! But things are not finished yet. Monte Carlo _Estimators_ are simple, this is what we know. AND, one more thing,\n",
        "\n",
        "**Monte Carlo Integration can be performed on arbitrarily high dimension.**\n",
        "\n",
        "Repeat it for three times, Monte Carlo Integration can be performed on arbitrarily high dimension,\n",
        "Monte Carlo Integration can be performed on arbitrarily high dimension,\n",
        "Monte Carlo Integration can be performed on arbitrarily high dimension.\n",
        "\n",
        "Reflected into the code, this says, the set in $\\mathbb{R}$ indicated by `low` and `high` can be switched into some unknown representation.\n",
        "\n",
        "To avoid spoilers on one of the most the interesting part of Computer Graphics, we'll not delve into this topic.\n",
        "Later, we'll have the chance to utilize this awesome method to imitate the real-world magic in our personal computers!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvYIF-nt7wj5"
      },
      "source": [
        "### Unbiased Variance Estimator\n",
        "\n",
        "To test your understanding of the above,\n",
        "we designed a simple exercise involving both Monte Carlo Integration and unbiased estimator design.\n",
        "\n",
        "Here it is! Design an **unbiased** variance estimator and implement it!\n",
        "To be specific, we asked you to design an estimator $\\theta$ for a random variable $X$,\n",
        "so that $\\mathbb{E}\\left[\\theta\\right] = \\mathrm{Var}(X)$.\n",
        "\n",
        "You are given $100$ i.i.d. samples of $X$, $\\left\\{X_i\\right\\}_{i=1}^{100}$.\n",
        "\n",
        "Hint:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "  \\mathrm{Var}(X)\n",
        "  & = \\mathbb{E}\\left[X^2\\right] - \\mathbb{E}^2\\left[X\\right] \\\\\n",
        "  & = \\int_{-\\infty}^{\\infty}{x^2 p(x) \\, \\mathrm{d} x} - \\mathbb{E}^2\\left[X\\right].\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "Answer:\n",
        "Design $\\theta$ so that (assume $n$ is even)\n",
        "\n",
        "$$\n",
        "  \\theta = \\left[\\dfrac{1}{n} \\sum_{i=1}^{n}X_i^2\\right] -\n",
        "  \\left[\n",
        "    \\left(\\dfrac{2}{n}\\sum_{i=1,3,\\cdots}^{n}X_i\\right)\n",
        "    \\left(\\dfrac{2}{n}\\sum_{i=2,4,\\cdots}^{n}X_i\\right)\n",
        "  \\right]\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhTEMmZb7wj6",
        "outputId": "49fab512-e96d-482f-e557-ae27f2cdcdc1"
      },
      "outputs": [],
      "source": [
        "def make_variance_estimator(rv: RandomVariable, n: int = 100) -> RandomVariable:\n",
        "    class Estimator(RandomVariable):\n",
        "        def __init__(self, n) -> None:\n",
        "            super().__init__()\n",
        "            self.n_ = n\n",
        "\n",
        "        def sample(self) -> float:\n",
        "            X = [rv.sample() for _ in range(self.n_)]\n",
        "            X = np.array(X)\n",
        "            return 1 / self.n_ * np.sum(X * X) - (\n",
        "                2 / self.n_ * np.sum(X[: (self.n_ // 2)])\n",
        "            ) * (2 / self.n_ * np.sum(X[(self.n_ // 2) :]))\n",
        "\n",
        "    return Estimator(n)\n",
        "\n",
        "\n",
        "print(\"normal calculated:\", make_variance_estimator(Normal(42, 4)).expect())\n",
        "print(\"       expected:\", 16)\n",
        "print(\"uniform calculated:\", make_variance_estimator(Uniform(0, 1)).expect())\n",
        "print(\"        expected\", 1 / 12.0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EHoafEg7wj7"
      },
      "source": [
        "### Combine them together\n",
        "\n",
        "This section serves as an appendix, to combine the previously mentioned methods together, to estimate this!\n",
        "\n",
        "$$\n",
        "  \\mathrm{Tr}(p \\rightarrow p') = \\exp\\left(-\\int_{0}^{t}{\\sigma(t) \\, \\mathrm{d} t}\\right)\n",
        "$$\n",
        "\n",
        "Which is the transmittance between two points. We might expect an _unbiased_ estimator on this.\n",
        "This formula is common in _Volume Rendering_, designing an efficient unbiased estimator of which is generally regarded as a hard task.\n",
        "\n",
        "But, we already have the necessary tools to accomplish this task, just combine the previous functions!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3pfkK3917wj8",
        "outputId": "fef50f5c-9863-468f-d9af-9bfcf2e3f464"
      },
      "outputs": [],
      "source": [
        "def make_taylor_transmittance_estimator(integral: NumericalIntegral):\n",
        "    estimator = integral.make_monte_carlo_estimator(Uniform)\n",
        "\n",
        "    class Estimator(RandomVariable):\n",
        "        def __init__(self) -> None:\n",
        "            super().__init__()\n",
        "\n",
        "        def sample(self) -> float:\n",
        "            return rr(lambda i: exp_series(func=estimator.sample, i=i))\n",
        "\n",
        "    return Estimator()\n",
        "\n",
        "\n",
        "m_integral = NumericalIntegral(func=lambda x: -func(x), low=0.0, high=4.5)\n",
        "tr_est_uniform = make_taylor_transmittance_estimator(m_integral)\n",
        "print(f\"groundtruth: {np.exp(m_integral.get_value()):.4f}\")\n",
        "print(f\"mean:        {tr_est_uniform.expect():.4f}\")\n",
        "print(f\"variance:    {make_variance_estimator(tr_est_uniform, 10).expect():.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfWR9O0a7wj8"
      },
      "source": [
        "### MCMC Methods\n",
        "\n",
        "This part is completely optional, but we recommend you to execute it and check its result comparing to previous method.\n",
        "It is based on [This paper](https://cs.dartmouth.edu/wjarosz/publications/novak14residual.pdf),\n",
        "you can refer to it for pseudo code.\n",
        "It only demonstrates the possibility of highly efficient method for the previous formula.\n",
        "\n",
        "Execute this code! Experience the power of the MCMC methods. You don't have to understand it for now.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IabxMlcr7wj9",
        "outputId": "94210f56-835d-4168-e697-137613d1fbfc"
      },
      "outputs": [],
      "source": [
        "def delta_tracking(integral: NumericalIntegral, max_f: float):\n",
        "    x = integral.low_\n",
        "    while True:\n",
        "        eta = np.random.uniform()\n",
        "        x -= np.log(1 - eta) / max_f\n",
        "        if x >= integral.high_:\n",
        "            break\n",
        "        epsilon = np.random.uniform()\n",
        "        if epsilon <= integral.func_(x) / max_f:\n",
        "            break\n",
        "    return x\n",
        "\n",
        "\n",
        "def make_delta_tracking_transmittance_estimator(\n",
        "    integral: NumericalIntegral,\n",
        ") -> RandomVariable:\n",
        "    max_f = integral.get_max()\n",
        "\n",
        "    class Estimator(RandomVariable):\n",
        "        def __init__(self) -> None:\n",
        "            super().__init__()\n",
        "\n",
        "        def sample(self) -> float:\n",
        "            return 1 if delta_tracking(integral, max_f) > integral.high_ else 0\n",
        "\n",
        "    return Estimator()\n",
        "\n",
        "\n",
        "def make_ratio_tracking_transmittance_estimator(\n",
        "    integral: NumericalIntegral,\n",
        ") -> RandomVariable:\n",
        "    max_f = integral.get_max()\n",
        "\n",
        "    class Estimator(RandomVariable):\n",
        "        def __init__(self) -> None:\n",
        "            super().__init__()\n",
        "\n",
        "        def sample(self) -> float:\n",
        "            x = integral.low_\n",
        "            T = 1\n",
        "            while True:\n",
        "                eta = np.random.uniform()\n",
        "                x -= np.log(1 - eta) / max_f\n",
        "                if x >= integral.high_:\n",
        "                    break\n",
        "                T *= 1 - integral.func_(x) / max_f\n",
        "            return T\n",
        "\n",
        "    return Estimator()\n",
        "\n",
        "\n",
        "def make_residual_tracking_transmittance_estimator(\n",
        "    integral: NumericalIntegral, f_c: float\n",
        ") -> RandomVariable:\n",
        "    class Estimator(RandomVariable):\n",
        "        def __init__(self, f_c) -> None:\n",
        "            super().__init__()\n",
        "            self.f_c_ = f_c\n",
        "            self.f_r_ = -optimize.minimize_scalar(\n",
        "                lambda x: -np.abs(integral.func_(x) - f_c),\n",
        "                bounds=[integral.low_, integral.high_],\n",
        "                method=\"bounded\",\n",
        "            ).fun\n",
        "\n",
        "        def sample(self) -> float:\n",
        "            x = integral.low_\n",
        "            Tc = np.exp(-self.f_c_ * (integral.high_ - integral.low_))\n",
        "            Tr = 1.0\n",
        "            while True:\n",
        "                eta = np.random.uniform()\n",
        "                x -= np.log(1 - eta) / self.f_r_\n",
        "                if x >= integral.high_:\n",
        "                    break\n",
        "                Tr *= 1 - (integral.func_(x) - self.f_c_) / self.f_r_\n",
        "            return Tc * Tr\n",
        "\n",
        "    return Estimator(f_c)\n",
        "\n",
        "\n",
        "tr_est_dt = make_delta_tracking_transmittance_estimator(integral)\n",
        "tr_est_rt = make_ratio_tracking_transmittance_estimator(integral)\n",
        "tr_est_rst_mean = make_residual_tracking_transmittance_estimator(\n",
        "    integral, integral.get_value() / (integral.high_ - integral.low_)\n",
        ")\n",
        "print(f\"groundtruth:  {np.exp(m_integral.get_value()):.4f}\")\n",
        "print(f\"our mean:     {tr_est_uniform.expect():.4f}\")\n",
        "print(f\"dt  mean:     {tr_est_dt.expect():.4f}\")\n",
        "print(f\"rt  mean:     {tr_est_rt.expect():.4f}\")\n",
        "print(f\"rst mean:     {tr_est_rst_mean.expect():.4f}\")\n",
        "print(f\"our variance: {make_variance_estimator(tr_est_uniform, 10).expect():.4f}\")\n",
        "print(f\"dt  variance: {make_variance_estimator(tr_est_dt, 10).expect():.4f}\")\n",
        "print(f\"rt  variance: {make_variance_estimator(tr_est_rt, 10).expect():.4f}\")\n",
        "print(f\"rst variance: {make_variance_estimator(tr_est_rst_mean, 10).expect():.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ynk4XBXZ7wj-"
      },
      "source": [
        "Their implementations seem easy. Well, yes!\n",
        "But the derivation is rather complex, and requires you to manage the previous concepts.\n",
        "You can refer to [this material](https://cs.dartmouth.edu/wjarosz/publications/novak14residual-supplemental1.pdf) for more information.\n",
        "We'll only prove _Ratio-Tracking_ here for you to entertain ;)\n",
        "\n",
        "Happy grinding, and welcome to Computer Graphics!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WnFjeikzZC8K"
      },
      "source": [
        "## Multiple Importance Sampling\n",
        "\n",
        "Remember _Monte Carlo Integration_ that we discussed previously? Consider a predetermined function to integrate, say $f(x) = x^2$, and put aside the fact that we can analytically integrate this function. Is there a special choice of random variable that would yield a more efficient estimator for the integral of $f$? Intuitively, we might guess that when $p(X) \\sim f$, we achieve the best estimator. This is indeed true, because $f(X) / p(X)$ becomes a constant. Let's explore this by checking it with some code.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lb37gWbtazYK",
        "outputId": "a8f0dd86-ddbd-461e-defa-48c11b024477"
      },
      "outputs": [],
      "source": [
        "class SquareImportanceSampler(RandomVariable):\n",
        "    def __init__(self, a: float, b: float):\n",
        "        self.a_ = a\n",
        "        self.b_ = b\n",
        "        self.integral_ = 1 / 3 * (b**3 - a**3)\n",
        "\n",
        "    def sample(self) -> float:\n",
        "        u = np.random.uniform(low=0.0, high=1.0)\n",
        "        return (self.a_**3 + u * 3 * self.integral_) ** (1 / 3)\n",
        "\n",
        "    def pdf(self, x: float) -> float:\n",
        "        return x**2 / self.integral_\n",
        "\n",
        "\n",
        "def print_sampler_report(func, sampler):\n",
        "    integral = NumericalIntegral(func=func, low=0.1, high=1.1)\n",
        "    est_uniform = integral.make_monte_carlo_estimator(Uniform)\n",
        "    est_importance = integral.make_monte_carlo_estimator(sampler)\n",
        "    print(f\"est_uniform:       {est_uniform.expect():.4f}\")\n",
        "    print(f\"est_yours:         {est_importance.expect():.4f}\")\n",
        "    print(f\"variance_uniform:  {make_variance_estimator(est_uniform, 10).expect():.4f}\")\n",
        "    print(\n",
        "        f\"variance_yours:    {make_variance_estimator(est_importance, 10).expect():.4f}\"\n",
        "    )\n",
        "\n",
        "\n",
        "print_sampler_report(lambda x: x**2, SquareImportanceSampler)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-c-akx5eNQu"
      },
      "source": [
        "As the results show, the importance sampler provides a perfect estimator with an accurate mean and zero variance. But is this technique generally applicable? Regrettably, the answer is no. The problem arises in two main aspects:\n",
        "\n",
        "- Typically, we cannot derive the perfect sampler because the underlying function $f$ is either unavailable or cannot be integrated and inverted. In rendering, we often design estimators that are _good enough_ but not perfect.\n",
        "- We need to use the same estimator for different $f$ functions! That's the key point. Especially in rendering, we usually have only one type of estimator, but the scene configuration can vary greatly. Even if we design an estimator that _works well_ for a specific scene, it is challenging to generalize it to different scenes.\n",
        "\n",
        "Let's try it out by creating a different function to integrate.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Egxq8UXHfzGj",
        "outputId": "a970d7f0-a401-4d6e-8a85-831a553205e5"
      },
      "outputs": [],
      "source": [
        "print_sampler_report(lambda x: 1 / x, SquareImportanceSampler)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TDkHIdqf-8I"
      },
      "source": [
        "Well, the importance sampler gives a much worse result than even the baseline. If you are designing a library to integrate user input, this is definitely not what you want: the input can come in any form, and being good at only a single function achieves nothing.\n",
        "\n",
        "Introducing _Multiple Importance Sampling_ (MIS), one of the **most powerful** techniques in rendering. Many scenes cannot be effectively rendered without MIS. Bidirectional Path Tracing relies entirely on MIS, serving as a method to construct _efficient samplers_ for specific cases. However, the idea behind which is surprinsingly simple, and can be given in a few lines.\n",
        "\n",
        "We make the statement clear. We are to design a single estimator. For different inputs, we construct different efficient (sub-)samplers, say, $X_1$ for $f_1$ and $X_2$ for $f_2$. The function to be integrated is randomly chosen from either $f_1$ or $f_2$ but you don't know which one is chosen. We first combine $X_1$ and $X_2$ linearly.\n",
        "\n",
        "$$\n",
        "  X = c_1 X_1 + c_2 X_2,\n",
        "$$\n",
        "\n",
        "where $c_1 + c_2 = 1$. We have\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "  \\mathbb{E}[X] & = c_1 \\mathbb{E}[X_1] + c_2 \\mathbb{E}[X_2] = I,\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "where $I$ is the result of the integral on $f$. This seems trivially correct, but one crucial observation appears when we expand the expectation into integral:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "  c_1 \\mathbb{E}[X_1] + c_2 \\mathbb{E}[X_2]\n",
        "  & = \\int_\\Omega c_1 \\dfrac{f(x)}{p_1(x)} p_1(x) \\, \\mathrm{d} x + \\int_\\Omega c_1 \\dfrac{f(x)}{p_2(x)} p_2(x) \\, \\mathrm{d} x \\\\\n",
        "  & = \\int_{\\Omega} (c_1 + c_2) f(x) \\, \\mathrm{d} x,\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "where $p_1(x)$ and $p_2(x)$ are the PDF of the random variable respectively. See where? As long as $c_1 + c_2 = 1$, the equation holds. So why not extending $c_1$ and $c_2$ to a function on $X$, to $c_1(x)$ and $c_2(x)$. This gives\n",
        "\n",
        "$$\n",
        "  \\mathbb{E}[x] = c_1(x) \\dfrac{f(x)}{p_1(x)} + c_2(x) \\dfrac{f(x)}{p_2(x)},\n",
        "$$\n",
        "\n",
        "where $\\forall x \\in \\Omega, c_1(x) + c_2(x) = 1$.\n",
        "\n",
        "The actual choice of the $c_1(x)$ and $c_2(x)$ can be quite subtle, because the constrain is sooo weak, only $c_1(x) + c_2(x) = 1$. Can we just leverage the PDFs to give a better $c_1(x)$ and $c_2(x)$? Veach presents a method called _power heuristic_, where\n",
        "\n",
        "$$\n",
        "  c_1(x) = \\dfrac{p_1(x)^2}{p_1(x)^2 + p_2(x)^2}, c_2(x) = \\dfrac{p_2(x)^2}{p_1(x)^2 + p_2(x)^2}.\n",
        "$$\n",
        "\n",
        "Let's pause and observe the formula. In power heuristic, evaluating $c_1(x)$ requires the evaluation of both $p_1(x)$ and $p_2(x)$ on $x$. For a certain $x$, when $X_1$ gives a higher probability, it weights $X_1$ more on the result, weakening the influence of the other estimator.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m1BJ_towpSwR",
        "outputId": "b11aa4fa-43b7-4c66-cbf7-5af1512f51fd"
      },
      "outputs": [],
      "source": [
        "class InverseImportanceSampler(RandomVariable):\n",
        "    def __init__(self, a: float, b: float):\n",
        "        self.a_ = a\n",
        "        self.b_ = b\n",
        "        self.integral_ = math.log(b) - math.log(a)\n",
        "\n",
        "    def sample(self) -> float:\n",
        "        u = np.random.uniform(low=0.0, high=1.0)\n",
        "        return self.a_ * math.exp(u * self.integral_)\n",
        "\n",
        "    def pdf(self, x: float) -> float:\n",
        "        return (1 / x) / self.integral_\n",
        "\n",
        "\n",
        "print_sampler_report(lambda x: 1 / x, InverseImportanceSampler)\n",
        "print(\"-----\")\n",
        "print_sampler_report(lambda x: x**2, InverseImportanceSampler)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IdF1UCLBuot9"
      },
      "source": [
        "As you can see, it gives good estimate on $1/x$ but not $x^2$. Let construct a multiple importance sampler on these two samplers!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JI-KEsfSuxDm",
        "outputId": "4b3db5fe-2b62-46a9-e186-40a1887258f3"
      },
      "outputs": [],
      "source": [
        "def make_multiple_importance_estimator(integral, rv_factory1, rv_factory2):\n",
        "    class Estimator(RandomVariable):\n",
        "        def __init__(self, rv_factory1, rv_factory2):\n",
        "            self.x1_ = rv_factory1(integral.low_, integral.high_)\n",
        "            self.x2_ = rv_factory2(integral.low_, integral.high_)\n",
        "\n",
        "        def sample(self) -> float:\n",
        "            x1 = self.x1_.sample()\n",
        "            p11 = self.x1_.pdf(x1)\n",
        "            p21 = self.x2_.pdf(x1)\n",
        "            x2 = self.x2_.sample()\n",
        "            p12 = self.x1_.pdf(x2)\n",
        "            p22 = self.x2_.pdf(x2)\n",
        "            c1 = p11**2 / (p11**2 + p21**2)\n",
        "            c2 = p22**2 / (p12**2 + p22**2)\n",
        "\n",
        "            return c1 * integral.func_(x1) / p11 + c2 * integral.func_(x2) / p22\n",
        "\n",
        "    return Estimator(rv_factory1, rv_factory2)\n",
        "\n",
        "\n",
        "def print_estimator_report(func):\n",
        "    integral = NumericalIntegral(func, low=0.1, high=1.1)\n",
        "    mis_estimator = make_multiple_importance_estimator(\n",
        "        integral, SquareImportanceSampler, InverseImportanceSampler\n",
        "    )\n",
        "    uniform_estimator = integral.make_monte_carlo_estimator(Uniform)\n",
        "    print(f\"est_uniform: {uniform_estimator.expect():.4f}\")\n",
        "    print(f\"est_mis:     {mis_estimator.expect():.4f}\")\n",
        "    print(\n",
        "        f\"variance_uniform: {make_variance_estimator(uniform_estimator, 10).expect():.4f}\"\n",
        "    )\n",
        "    print(\n",
        "        f\"variance_mis:     {make_variance_estimator(mis_estimator, 10).expect():.4f}\"\n",
        "    )\n",
        "\n",
        "\n",
        "print_estimator_report(func=lambda x: x**2)\n",
        "print(\"-----\")\n",
        "print_estimator_report(func=lambda x: 1 / x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5_DjYtt96de"
      },
      "source": [
        "## Metropolis-Hastings algorithm\n",
        "\n",
        "Though this type of method do belongs to MCMC methods before, we also have a unique way to cut into this method.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZVLwnfaEEh96",
        "outputId": "764fcd17-e800-4f66-841e-3987d5feb6b8"
      },
      "outputs": [],
      "source": [
        "def make_metropolis_estimator(func, low, high):\n",
        "    class Estimator(RandomVariable):\n",
        "        def __init__(self, a: float, b: float):\n",
        "            self.a_ = a\n",
        "            self.b_ = b\n",
        "            self.func_ = func\n",
        "            self.sigma_ = 0.3\n",
        "            self.x_ = low + (high - low) * np.random.random()\n",
        "\n",
        "        def sample(self):\n",
        "            y = self.propose(self.x_)\n",
        "            if self.accept(self.x_, y):\n",
        "                self.x_ = y\n",
        "            return self.x_\n",
        "\n",
        "        def propose(self, x):\n",
        "            a = (self.a_ - x) / self.sigma_\n",
        "            b = (self.b_ - x) / self.sigma_\n",
        "            return truncnorm.rvs(a, b, loc=x, scale=self.sigma_)\n",
        "\n",
        "        def accept(self, x, y):\n",
        "            # calculate a(x -> y)\n",
        "            if y < self.a_ or y >= self.b_:\n",
        "                return False\n",
        "\n",
        "            fy = self.func_(y)  # f(y)\n",
        "            fx = self.func_(x)  # f(x)\n",
        "            acceptance = min(1.0, fy / fx)\n",
        "\n",
        "            return np.random.random() <= acceptance\n",
        "\n",
        "    return Estimator(low, high)\n",
        "\n",
        "\n",
        "def func(x):\n",
        "    return norm.pdf(x, loc=0.4, scale=0.1) + 0.2 + norm.pdf(x, loc=0.8, scale=0.05)\n",
        "\n",
        "\n",
        "estimators = []\n",
        "for i in range(8):\n",
        "    estimators.append(make_metropolis_estimator(func=func, low=0.1, high=1.1))\n",
        "for i in range(2000):\n",
        "    estimators[i % len(estimators)].sample()  # burn in\n",
        "\n",
        "inv_integral = 0.0\n",
        "lst = []\n",
        "n_samples = 5000\n",
        "for i in range(n_samples):\n",
        "    s = estimators[i % len(estimators)].sample()\n",
        "    inv_integral += 1.0 / func(s)\n",
        "    lst.append(s)\n",
        "\n",
        "print(1.0 / (inv_integral / n_samples))\n",
        "\n",
        "integral = NumericalIntegral(func=func, low=0.1, high=1.1)\n",
        "est_uniform = integral.make_monte_carlo_estimator(Uniform)\n",
        "print(integral.get_value())\n",
        "print(est_uniform.expect(n=n_samples))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "QFZWgvT1NopT",
        "outputId": "b4aea17b-74f1-4891-a976-1b4085a7f9b9"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "counts, bins, patches = plt.hist(lst, bins=200, edgecolor=\"black\")\n",
        "\n",
        "plt.title(\"Distribution of Values in Array\")\n",
        "plt.xlabel(\"Value\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "\n",
        "x = np.linspace(0.1, 1.1, 1000)\n",
        "y = np.array([func(xi) for xi in x]) * 16  # chosen scale factor\n",
        "plt.plot(x, y, \"r-\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sif4NRlDD8X5"
      },
      "source": [
        "## Epilogue\n",
        "\n",
        "We have to admit, the topic of _rendering_ is far from over, and the math is already becoming harder.\n",
        "But one thing to mention here, so far, the math is _concrete_. We are performing integrals in trivial sets or ranges.\n",
        "Once you figure out the concept through those (elaborately designed) interfaces, we can proceed to the next level,\n",
        "to grind through those abstractly defined equations and integrals!\n",
        "This is where the difficulty for neophyte in rendering comes from.\n",
        "\n",
        "Please notice, we are entering a brand new mathematically defined world after this lab, not the one that you are familiar with.\n",
        "We encourage you to ask yourself questions, for example, random variables are defined on _sample space_, so how do we design an estimator on this integral;\n",
        "what is **dummy variable** in integral; what do we really mean by integrating on solid angle?\n",
        "To build a comprehensive and mathematically correct path tracer, you should be able to ask and answer these questions.\n",
        "If you encounter any difficulties, reflect your model of abstraction, and we are happy to discuss these things with you!\n",
        "\n",
        "There are some good resources that worth reading. One we'd like to recommend here, aside from Physically Based Rendering, is\n",
        "ROBUST MONTE CARLO METHODS FOR LIGHT TRANSPORT SIMULATION. Check this [meme](https://twitter.com/yiningkarlli/status/1545186689894584320) :)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
